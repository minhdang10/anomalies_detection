{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T01:36:33.610795Z",
     "start_time": "2021-10-09T01:36:33.604495Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#Job Parameters\n",
    "job_id='11'\n",
    "job_name= 'PACKAGECOUNT_FACILITY_DELIVERY'\n",
    "\n",
    "# #Snowflake Input Credentials\n",
    "user_id='xxxx'\n",
    "passcode='xxxx'\n",
    "WAREHOUSE='FUSION_CS_DWH'\n",
    "snowflake_account='xxxx'\n",
    "\n",
    "#Snowflake Output Parameters\n",
    "snowflake_output_database = 'FUSION_FDR_DB'\n",
    "snowflake_output_schema = 'FUSION_ANOMOLY_DEV_SCHEMA'\n",
    "snowflake_output_table = 'ANOMALY_LIVE_TIMESERIES'\n",
    "\n",
    "#Cloudwatch Logs And Alert SNS ARN\n",
    "sns_arn = 'xxxx'\n",
    "log_group_name = 'xxxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# SETUP LOGGER\n",
    "###################\n",
    "import time\n",
    "import boto3\n",
    "import traceback\n",
    "\n",
    "sns = boto3.client(\"sns\", region_name='us-west-2')\n",
    "logs = boto3.client(\"logs\", region_name='us-west-2')\n",
    "next_sequence_token = ''\n",
    "\n",
    "\n",
    "log_stream_name= 'Package Count Facility Scan Event Tracking' + str(int(time.time()))\n",
    "\n",
    "params = [sns, logs, sns_arn, log_group_name, log_stream_name, next_sequence_token]\n",
    "\n",
    "def publish_multi_message(params, error_msg):\n",
    "    sns_arn = params[2]\n",
    "    sns = params[0]\n",
    "    message = {\n",
    "            # Change the subject by replacing with the appropriate name of the anomaly\n",
    "            \"subject\":\"Anomaly Package Count Scan Event Failed\",\n",
    "            \"email_message\":\"Anomaly Package Count Scan Event Failed. Please look into the logs.\" + \"\\nHere is the error msg:\\n \" + error_msg \n",
    "        }\n",
    "    response = sns.publish(\n",
    "        TopicArn=sns_arn,\n",
    "        Message=message[\"email_message\"], \n",
    "        Subject=message[\"subject\"]\n",
    "        )\n",
    "    message_id = response['MessageId']\n",
    "    return message_id\n",
    "\n",
    "def create_log_stream(params):\n",
    "    log_group_name = params[3]\n",
    "    log_stream_name = params[4]\n",
    "    logs = params[1]\n",
    "\n",
    "    response = logs.create_log_stream(\n",
    "        logGroupName=log_group_name,\n",
    "        logStreamName=log_stream_name\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def put_log_events(params, message):\n",
    "    log_group_name = params[3]\n",
    "    log_stream_name = params[4]\n",
    "    next_sequence_token = params[-1]\n",
    "    logs = params[1]\n",
    "\n",
    "    if len(next_sequence_token):\n",
    "        response = logs.put_log_events(\n",
    "            logGroupName=log_group_name,\n",
    "            logStreamName=log_stream_name,\n",
    "            logEvents=[{\n",
    "                'timestamp': int(time.time() * 1000),\n",
    "                'message': message\n",
    "            }],\n",
    "            sequenceToken=next_sequence_token\n",
    "        )\n",
    "        params[-1] = response[\"nextSequenceToken\"]\n",
    "    else:\n",
    "        response = logs.put_log_events(\n",
    "            logGroupName=log_group_name,\n",
    "            logStreamName=log_stream_name,\n",
    "            logEvents=[{\n",
    "                'timestamp': int(time.time() * 1000),\n",
    "                'message': message\n",
    "            }]\n",
    "        )\n",
    "        params[-1] = response[\"nextSequenceToken\"]\n",
    "    return params\n",
    "\n",
    "def log_and_publish_to_cloud(params):\n",
    "    log_msg = \"Exception Occurred!!!\"\n",
    "    params = put_log_events(params, log_msg)\n",
    "    \n",
    "    error_msg = traceback.format_exc()\n",
    "    params = put_log_events(params, error_msg)\n",
    "    message_id = publish_multi_message(params, error_msg)\n",
    "    return params\n",
    "\n",
    "create_log_stream(params)\n",
    "\n",
    "\"\"\"\n",
    "params = put_log_events(params, \"Uploading is completed !!!\")\n",
    "except Exception as e:\n",
    "    params = log_and_publish_to_cloud(params)\n",
    "    raise e\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #import packages\n",
    "    params = put_log_events(params, \"Importing the packages\")\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from datetime import datetime, timedelta\n",
    "    import pytz\n",
    "    import snowflake.connector\n",
    "    import itertools\n",
    "    from prophet import Prophet\n",
    "    import matplotlib.pyplot as plt\n",
    "    import time\n",
    "    from tqdm import *\n",
    "    import warnings\n",
    "except Exception as e:\n",
    "    params = put_log_events(params, \"Error occured while importing the packages\")\n",
    "    params = log_and_publish_to_cloud(params)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Parameters \n",
    "#total days\n",
    "test_last_days = 31\n",
    "\n",
    "# length of training period (days)\n",
    "training_period=test_last_days-1\n",
    "\n",
    "# training interval increment (days)\n",
    "interval=1\n",
    "\n",
    "# window between checkpoint (hours)\n",
    "window='6H'\n",
    "\n",
    "#number of forecast (based on windows between checkpoint)\n",
    "number_of_forecast = 24/int(window[0])\n",
    "\n",
    "#group-by variable for printing during training\n",
    "category_1='Facility'\n",
    "category_2='Event'\n",
    "\n",
    "#list of confidence interval\n",
    "confidence_interval=[0.80,0.95]   \n",
    "\n",
    "# Format output file\n",
    "monitor_name = 'PackageCount_Facility_Delivery'\n",
    "lob = 'Tracking'\n",
    "version = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start and end date filter for data query\n",
    "et=datetime.now(pytz.timezone('US/Central')).date()\n",
    "st=et-timedelta(days=test_last_days)\n",
    "\n",
    "#query from Snowflake\n",
    "query = '''\n",
    "            select date_trunc('HOUR',eventrptdatetime) as Event_Time,\n",
    "            eventfacilityname AS Category_1,trackingeventkey AS Category_2,count(1) AS Actual\n",
    "\n",
    "            from \"FDR_DWH_DB_PROD\".\"RETURNS\".\"REVTRACKINGEVENT\" \n",
    "            where date(eventrptdatetime) >='''+\"'\"+str(st)+\" 00:00:00'\"+'''\n",
    "            and date(eventrptdatetime) <= '''+\"'\"+str(et)+\" 00:00:00'\"+'''\n",
    "            and eventfacilityname in ('IND3','EWR2','ATL4','ONT1','RNO2','DFW5') \n",
    "            and trackingeventkey in (7,14,33,335,334,333) \n",
    "            group by Event_Time,eventfacilityname,trackingeventkey;\n",
    "            '''\n",
    "#             and eventfacilityname in ('IND3','EWR2','ATL4','ONT1','RNO2','DFW5','CVG1','ORD2','BWI1','MCO1') \n",
    "#             and trackingeventkey in (7,14,33,335,334,333) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append(\"/path/to/file/\")\n",
    "sys.path.append(\"/10_PACKAGECOUNT_FACILITY_DELIVERY/\")\n",
    "#import from functions.py\n",
    "from functions import createConnection,extractData,exploratory_data_analysis,preprocessing,modeling_production_live,output_cleaning,formatted_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T01:57:38.481097Z",
     "start_time": "2021-10-09T01:57:36.484428Z"
    }
   },
   "outputs": [],
   "source": [
    "#connect to Snowflake and query data\n",
    "try:\n",
    "    params = put_log_events(params, \"Creating the snowflake connector\")\n",
    "    createConnection(user_id,passcode)\n",
    "except Exception as e:\n",
    "    params = log_and_publish_to_cloud(params)\n",
    "    raise e\n",
    "\n",
    "try:\n",
    "    params = put_log_events(params, \"Extracting the data\")\n",
    "    df=extractData(st,et,query)\n",
    "except Exception as e:\n",
    "    params = log_and_publish_to_cloud(params)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA\n",
    "try:\n",
    "    params = put_log_events(params, \"Performing the EDA\")\n",
    "    exploratory_data_analysis(df)\n",
    "except Exception as e:\n",
    "    params = log_and_publish_to_cloud(params)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing data to train\n",
    "try:\n",
    "    params = put_log_events(params, \"Preprocessing the Data\")\n",
    "    df=preprocessing(df,window)\n",
    "except Exception as e:\n",
    "    params = log_and_publish_to_cloud(params)\n",
    "    raise e\n",
    "\n",
    "#get list of category_1 and 2\n",
    "category_1_list=list(df['CATEGORY_1'].unique())\n",
    "category_2_list=list(df['CATEGORY_2'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modeling\n",
    "try:\n",
    "    params = put_log_events(params, \"Running the model\")\n",
    "    final_output=modeling_production_live(category_1,category_2,confidence_interval,category_1_list,category_2_list,df,window,training_period,interval,number_of_forecast)\n",
    "except Exception as e:\n",
    "    params = log_and_publish_to_cloud(params)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the Prophet output\n",
    "try:\n",
    "    params = put_log_events(params, \"Cleaning the output\")\n",
    "    cleaned_output=output_cleaning(final_output,df)\n",
    "except Exception as e:\n",
    "    params = log_and_publish_to_cloud(params)\n",
    "    raise e\n",
    "\n",
    "cleaned_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format CSV file\n",
    "try:\n",
    "    params = put_log_events(params, \"Formatting the cleaned output\")\n",
    "    results=formatted_csv(cleaned_output,monitor_name,lob,version)\n",
    "except Exception as e:\n",
    "    params = log_and_publish_to_cloud(params)\n",
    "    raise e\n",
    "    \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results structure\n",
    "results.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-09T01:58:45.055Z"
    }
   },
   "outputs": [],
   "source": [
    "from snowflake.connector.pandas_tools import pd_writer\n",
    "import os\n",
    "\n",
    "def write_to_snowflake(data, params):\n",
    "    try:\n",
    "        job_run_time=datetime.today()\n",
    "        print(\"Writing to Snowflake\")\n",
    "        \n",
    "        #Append Job Parameters in OutputDataFrame\n",
    "        data.insert(0,'JOB_ID',job_id)\n",
    "    #     data.insert(1,'RUN_DATE',datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    #     data.insert(2,'ANOMALY_NAME',job_name)\n",
    "\n",
    "        ctx = snowflake.connector.connect(\n",
    "        user= user_id,\n",
    "        password= passcode,\n",
    "        account= snowflake_account\n",
    "        )\n",
    "        #create a cursor object.\n",
    "        cs = ctx.cursor()\n",
    "\n",
    "        #Use Snowflake_Output_Schema\n",
    "        cs.execute(\"USE WAREHOUSE \" + WAREHOUSE)\n",
    "        cs.execute(\"USE DATABASE \" + snowflake_output_database)\n",
    "        cs.execute(\"USE SCHEMA \" + snowflake_output_schema)\n",
    "        \n",
    "        #Create File Format\n",
    "        Format_name = \"ANOMOLY_FILE_FORMAT\"\n",
    "        fileFormatQuery = 'CREATE OR REPLACE FILE FORMAT ' +Format_name+ ' TYPE = \\'CSV\\' NULL_IF = (\\'\\') EMPTY_FIELD_AS_NULL = TRUE field_delimiter = \\'|\\''\n",
    "        cs.execute(fileFormatQuery)\n",
    "        inputFileName = str(job_id)+'_'+str(job_name)+'_'+datetime.now().strftime('%Y%m%d_%H%M%S')+'.csv'\n",
    "        params = put_log_events(params, \"Input Filename is \"+inputFileName)\n",
    "        \n",
    "        #Create Local Job Output Directory\n",
    "        os.system(\"mkdir -p job_output\")\n",
    "        \n",
    "        # save file to local\n",
    "        data.to_csv('job_output/'+inputFileName,index=False,header=False,sep='|')\n",
    "        \n",
    "        # upload to stage\n",
    "        uploadToStageQuery = 'put file://job_output/'+inputFileName+' @%ANOMALY_LIVE_TIMESERIES;'\n",
    "        params = put_log_events(params, \"Query to upload the file to the stage table \" + uploadToStageQuery)\n",
    "        cs.execute(uploadToStageQuery)\n",
    "        params = put_log_events(params, \"Data Uploaded to the stage table is with filename \" + inputFileName)\n",
    "        \n",
    "        \n",
    "        # copy to table\n",
    "        copyIntoQuery  = 'COPY INTO \"'+snowflake_output_table+'\" FROM @\"%ANOMALY_LIVE_TIMESERIES\" FILE_FORMAT = \"'+Format_name+'\"'\n",
    "        params = put_log_events(params, copyIntoQuery)\n",
    "        cs.execute(copyIntoQuery)\n",
    "        params = put_log_events(params, \"Data Uploaded to the table \" + snowflake_output_table)\n",
    "\n",
    "    except Exception as e:\n",
    "        params = put_log_events(params, \"Exception occurred!!!\")\n",
    "        raise e\n",
    "    finally:       \n",
    "        # Remove Staged Files from Staged\n",
    "        removeStagedQuery = 'rm @%ANOMALY_LIVE_TIMESERIES/'+str(inputFileName) + '.gz'\n",
    "        cs.execute(removeStagedQuery)\n",
    "        cs.close()\n",
    "      \n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-09T01:58:45.238Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    params = put_log_events(params, \"Writing the data to the snowflake\")\n",
    "    write_to_snowflake(results, params) \n",
    "except Exception as e:\n",
    "    params = log_and_publish_to_cloud(params)\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "5bd3978c0ef2b6dccb5d0e20f74bb363e893c2584edfba87be105ab3c7b664bd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
